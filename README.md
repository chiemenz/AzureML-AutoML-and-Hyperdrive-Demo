# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
The Dataset seems to be based on the Kaggle Bank Marketing Dataset
https://www.kaggle.com/janiobachmann/bank-marketing-dataset

In total it contain data about bank customers (In total 21 columns) including the success of past marketing campaigns.

Numeric columns:
balance, day, age, duration, campaign, pdays, emp.var.rate, cons.price.idx, euribor3m, nr.employed

Categorical columns:
month, dayofweek, default, job, loan, marital status, education, housing, contact, previous, poutcome

with a label column: success of marketing campaign (yes/no)

The goal of the Dataset is to predict the success of future marketing campaigns based on the outcomes of past marketing campaings
by taking into account the features of target customers. 

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
The best performing model was a LightGBM (gradient boosting machine) with MaxAbsScaler preprocessing. 
It has an overall accuracy of 0.91596 and was found by using AutoML on the cleaned Dataset.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
For the general ML-Pipeline see:
I. As a first step, the Dataset is loaded 

II. Data preprocessing/cleanup step:
1. na columns are dropped
2. marital, default, housing, loan and poutcome are one-hot encoded
3. Also the contact, education columns are one-hot encoded by dummy variables
4. The months are mapped to numbers 1 - 12 and the 
5. Weekdays are mapped to numbers 1- 7
6. The target label column is one-hot encoded 1=success 0=failure of marketing campaign

III. The Dataset is split into a training and test set with sklear train_test_split (a test size of 20%  is used)
IV. A regularization constant and maximum number of iterations are defined as hyperparameters
V. A logistic Regression Classifier is fitted (taking into account the hyperparameters
VI. The accuracy of the Classifier is evaluated based on the test set

Under the Hyperparamer search conditions, an optimal classifier is found based on varying the hyperparameters of the model and evaluating 
the model performance with those specific hyperparameters.
E.g. for the Logistic Regression Classfier, the l2 regularization strengh is varied and the maximum number of iterations. 

For Hyperparameter search a Random Search is employed. As a result a random combination of the hyperparameters C and max_iter are sampled
based on the provided constraints.
E.g. a loguniform range from -3 to 2 is defined to find optimal parameters for the constant C (10^-3 - 10^2)
And a choice set is defined for the maximum number of iterations choice(1,5,10,20,30,40,50,80,100,200,400,800,1000)

For each iteration of hyperparameter search a different combination is sampled e.g. C = 3*10^-1 and max_iter = 80
Those parameters are the set at step IV and setps IV - VI are run to find the accuracy of the Logistic Regression model with those hyperparameters. 
**What are the benefits of the parameter sampler you chose?**
In contrast to RAndom Search, Grid Search is an exhaustive search method across the hyperparameter space, which sequentially tries every combination of hyperparameters.
Due to the randomess it is efficiently exploring the whole of the search space more rapidly. By setting up some stopping criterion, the random search can stop exactly at 
the point in time once the Stopping Criterion is reached. By definition random search makes random steps across the search space as a whole and in contrast to 
Grid search which explores each parameter space region sequentially it can visit different parameter space regions more rapidly and will stop once some subregion satisfies the stopping criterion, while Grid-search is still searching in some local subspace far away from this random subspace.
**What are the benefits of the early stopping policy you chose?**


## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
